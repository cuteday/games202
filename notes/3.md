## [GAMES 202] Lecture Notes 3

### Shadow Mapping

A **2-pass algorithm**: 
light pass: recording depth, render-to-texture (in a separate framebuffer);  
camera pass: reproject camera-visible fragments to get depth from light.

An **image-space** algorithm: no need of knowledge of scene geometry.
Cons: self-occlusion and aliasing (caused by low precision and resolution)

Implementation details: whether shadow texture comparing real camera-space $z$ (i.e. orthographic projecting) or perspective projected $z'$ is OK, as long as consistent. The orthographic projecting is often used in directional lights.

<img src="https://learnopengl.com/img/advanced-lighting/shadow_mapping_projection.png" alt="Shadow mapping difference between orthographic and perspective projection." style="zoom:50%;" />

#### Issues

##### Self occlusion

z-fighting (precision problem, for assuming a constant depth within the area in a pixel, which may have a varing depth value in reality.)

**Solution #1**: adding a bias $\text{eps}$ when comparing depth values ($\text{eps}$ may adaptive to light incident angle); Cons: detatched shadow (*peter panning*) happens when $eps$ is too big.

**Solution #2**: second-depth shadow mapping using midpoint between first and second depth value (may get with face-culling?) in shadow mapping. Cons: the watertight-object requirement and large overhead.
Other solutions: over sampling,  ...

##### Aliasing

when projecting shadow of a small object to a far-away surface...
**Solution**: Cascaded Shadow Mapping, CSM, PCF, ...

**Multiple light sources?** $N \times$ shadow maps and $\approx N \times$ computational cost! (optimization methods exists)

### Approximations in RTR

#### **An important approximation**:

$$
\int_\Omega f(x)g(x)dx \approx \frac {\int_\Omega f(x)dx}{\int_\Omega dx} \cdot \int_\Omega g(x)dx
$$

It's more accurate when: (at least 1 condition is achieved)
1 the integrated area is small (*small support*)
2 the integrand is smooth

**Applications:**
Decoupling the visibility term (separate process: first shading, then do shadow mapping).
$$
\begin{align}
L_o(p,w_o) & =\int_{S^2}L_i(p, w_i)f(p,w_i, w_o)cos\theta_iV(\text{p}, \omega_i)dw_i \\
& \approx \frac {\int_\Omega V(p, w_i) dw_i}{\int_\Omega dw_i} \cdot\int_{S^2}L_i(p, w_i)f(p,w_i, w_o)cos\theta_idw_i
\end{align}
$$
**Constraints:**
1 When small support: point / directional lights:
2 Smooth integrand: diffuse bsdf / <u>constant radiance area lighting</u>

### PCSS: Percentage Closing Soft Shadow

#### PCF: Percentage Closer Filtering

[source from nvidia](http://download.nvidia.com/developer/presentations/2005/SIGGRAPH/Percentage_Closer_Soft_Shadows.pdf)
Anti-aliasing the shadow's edges: filter the **shadow map depth-comparing results**, around each point, average the visibility results.
Filtering size: small->shaper, lager->softer

**Issue:** 
sometimes the softening effect went to far. (especially when the shadow is close to the blocker, which makes the unrealistic visual effect).
Notice the noisy shadow nearing 202chan's hair:

<img src="https://cutesail.com/wp-content/uploads/2021/04/pcf_202chan.png" alt="pcf_202chan" style="zoom:33%;" />

#### From PCF to PCSS: the adaptive filtering PCF

Guess: very large filter will achieve soft shadows?
Key observation: the more distant from blocker, the softer the shadow
Conclusion: filter size -> **relative average** projected blocker depth

<img src="https://cutesail.com/wp-content/uploads/2021/03/pcss.png" alt="截屏2021-03-24 下午11.57.36" style="zoom: 20%;" />
$$
w_{Penumbra} = \frac{(d_{Receiver}-d_{Blocker})\cdot w_{light}}{d_{Blocker}}
$$
**Steps:**
1 Blocker Search: average **blocker depth $d_\text{Blocker}$ from shadow map** (note: [1] the shadow map by taking the area light as a point light; [2] the search area can depends on the area light size and receiver's distance from light. )
2 Penumbra Estimation: use the ave blocker depth to determine filter size $w_\text{penumbra}$
3 do PCF using the adaptive sized filter

